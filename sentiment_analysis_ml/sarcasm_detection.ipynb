{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.gpu.set_per_process_memory_fraction(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# try: \n",
    "#   tf.config.set_logical_device_configuration( \n",
    "#     physical_devices[0], \n",
    "#     [tf.config.LogicalDeviceConfiguration(memory_limit=5000), \n",
    "#     ]) \n",
    " \n",
    "# #   logical_devices = tf.config.list_logical_devices('GPU') \n",
    "# #   assert len(logical_devices) == len(physical_devices) + 1 \n",
    " \n",
    "# #   tf.config.set_logical_device_configuration( \n",
    "# #     physical_devices[0], \n",
    "# #     [tf.config.LogicalDeviceConfiguration(memory_limit=10), \n",
    "# #      tf.config.LogicalDeviceConfiguration(memory_limit=10)]) \n",
    "# except Exception as e: \n",
    "#   print(\"Invalid device or cannot modify logical devices once initialized.\") \n",
    "#   print(e) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: \n",
    "#   tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "#   assert tf.config.experimental.get_memory_growth(physical_devices[0]) \n",
    "# except: \n",
    "#   # Invalid device or cannot modify virtual devices once initialized. \n",
    "#   pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample code to show tokenization of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentences = [\n",
    "#     'i love my dog',\n",
    "#     'I, love my cat',\n",
    "#     'You love my dog!',#focus on how tokenizer didnt create a new word for dog!\n",
    "#     'do you think my dog is amazing?'# adding new words\n",
    "    \n",
    "# ]\n",
    "\n",
    "# tokenizer = Tokenizer(num_words = 100,oov_token = \"<OOV>\")# adding out of vocabulary to ensure we do not lose the len of sentence for the words it hasnt seen before\n",
    "\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# word_index = tokenizer.word_index\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences = tokenizer.texts_to_sequences(sentences)# simply shows texts interpreted as numbers generated by tokenizer\n",
    "# #simple tensor can not handle unequal length sentences\n",
    "# #teherfor we'll have to pad the sentences to ensure an equal length\n",
    "# #or USE RAGGED TENSOR\n",
    "# padded = pad_sequences(sequences,maxlen=5)#if len is more than this specify how to truncate i.e. pre or post\n",
    "# #padding can be done both pre and post by  usin padding ='post' or 'pre' default is pre\n",
    "# #also truncatin ='post' or 'pre' can be used default is pre with maxlen\n",
    "\n",
    "# print('\\nWord_index =',word_index)\n",
    "# print(\"\\nSequences =\",sequences)\n",
    "# print(\"\\nPadded Sequences:\",padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = [\n",
    "#     'i really love my dog',\n",
    "#     'my dog loves my mantee'\n",
    "# ]\n",
    "# test_seq= tokenizer.texts_to_sequences(test_data)\n",
    "# padded_test_seq = pad_sequences(test_seq,maxlen=10)\n",
    "# print (padded_test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment (sarcasm) analysis on news head lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-28 21:34:24--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4002:80a::2010, 172.217.26.240\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4002:80a::2010|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5643545 (5.4M) [application/json]\n",
      "Saving to: ‘/tmp/sarcasm.json’\n",
      "\n",
      "/tmp/sarcasm.json    49%[========>           ]   2.68M  62.4KB/s    eta 29s    ^C\n"
     ]
    }
   ],
   "source": [
    "#importing data\n",
    "\n",
    "!wget --no-check-certificate\\\n",
    "   https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
    "   -O /tmp/sarcasm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type = 'post'\n",
    "padding_type='post'\n",
    "oov_token = \"<OOV>\"\n",
    "training_size = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Unterminated string starting at: line 13321 column 2 (char 2799176)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4061d506edcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/sarcasm.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdatastore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#simply opening json file and storing it in python list format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 13321 column 2 (char 2799176)"
     ]
    }
   ],
   "source": [
    "with open(\"/tmp/sarcasm.json\",'r') as f:\n",
    "    datastore = json.load(f)\n",
    "    #simply opening json file and storing it in python list format\n",
    "    \n",
    "sentences =[]\n",
    "labels =[]\n",
    "\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= {'sentences':sentences,'labels':labels}\n",
    "df = pd.DataFrame(d)\n",
    " \n",
    "df['labels'].value_counts()\n",
    "df.head()\n",
    "is_sarcastic = df[df['labels']==1]\n",
    "not_sarcastic = df[df['labels']==0]\n",
    "is_sarcastic.shape,not_sarcastic.shape\n",
    "\n",
    "not_sarcastic= not_sarcastic.sample(is_sarcastic.shape[0])\n",
    "\n",
    "df =is_sarcastic.append(not_sarcastic,ignore_index=True)\n",
    "df =df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#The frac keyword argument specifies the fraction of rows to return in the random sample, so frac=1 means return all rows (in random order).\n",
    "#Here, specifying drop=True prevents .reset_index from creating a column containing the old index entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df[0:training_size]\n",
    "testing_data = df[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\n",
    "#initailises tokeniser with vocab size and oov token\n",
    "tokenizer.fit_on_texts(training_data['sentences'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_data['sentences'])\n",
    "training_padded = pad_sequences(training_sequences, maxlen= max_length,padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_data['sentences'])\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen= max_length,padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this block to get it to work with tensorflow 2.x\n",
    "\n",
    "training_padded =np.array(training_padded)\n",
    "testing_padded =np.array(testing_padded)\n",
    "training_labels =np.array(training_data['labels'])\n",
    "testing_labels =np.array(testing_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n",
    "    #Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "    #use some kind of mapping\n",
    "    #https://keras.io/layers/embeddings/     for reference\n",
    "    #Input shape : 2D tensor with shape: (batch_size, sequence_length).\n",
    "    #Output shape : 3D tensor with shape: (batch_size, sequence_length, output_dim).\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/deity/Documents/sentiment_analysis/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callbacks = tf.keras.callbacks.TensorBoard(log_dir=path,profile_batch=0)\n",
    "                                                       #histogram_freq=1\n",
    "                                                      #write_graph=True,\n",
    "                                                       #write_images=True,\n",
    "                                                       #update_freq='epoch',\n",
    "                                                       #embeddings_freq=5,\n",
    "                                                     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "history = model.fit(training_padded,training_labels,epochs = num_epochs,\n",
    "                   validation_data=(testing_padded,testing_labels),verbose =2,\n",
    "                   callbacks=[tensorboard_callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Sentimentanlysismodel'\n",
    "pickle.dump(model,open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/deity/Documents/sentiment_analysis/mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history,string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string,'val_'+string])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history,\"accuracy\")\n",
    "plot_graphs(history,\"loss\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "def decode_sentence(text):\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text])\n",
    "\n",
    "print(decode_sentence(training_padded[2]))\n",
    "print(df['sentences'][2])\n",
    "print(df['labels'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) \n",
    "#shape : (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8ff449fa07744cd3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8ff449fa07744cd3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen =[\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "sentence = sen\n",
    "print (type(sentence))\n",
    "sequences = Tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
